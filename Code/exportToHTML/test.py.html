<html>
<head>
<title>test.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.ln { color: #999999; font-weight: normal; font-style: normal; }
.s0 { color: rgb(0,0,0); }
.s1 { color: rgb(0,0,128); font-weight: bold; }
.s2 { color: rgb(128,128,128); font-style: italic; }
.s3 { color: rgb(0,128,128); font-weight: bold; }
</style>
</head>
<BODY BGCOLOR="#ffffff">
<TABLE CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<TR><TD><CENTER>
<FONT FACE="Arial, Helvetica" COLOR="#000000">
test.py</FONT>
</center></TD></TR></TABLE>
<pre>
<span class="s0"> 
</span><span class="s1">class </span><span class="s0">LSTM(RNNBase): 
    </span><span class="s2">r&quot;&quot;&quot;Applies a multi-layer long short-term memory (LSTM) RNN to an input 
    sequence. 
 
 
    For each element in the input sequence, each layer computes the following 
    function: 
 
    .. math:: 
 
            \begin{array}{ll} 
            i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\ 
            f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\ 
            g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\ 
            o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\ 
            c_t = f_t c_{(t-1)} + i_t g_t \\ 
            h_t = o_t \tanh(c_t) 
            \end{array} 
 
    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell 
    state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}` 
    is the hidden state of the previous layer at time `t-1` or the initial hidden 
    state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`, 
    :math:`o_t` are the input, forget, cell, and output gates, respectively. 
    :math:`\sigma` is the sigmoid function. 
 
    Args: 
        input_size: The number of expected features in the input `x` 
        hidden_size: The number of features in the hidden state `h` 
        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2`` 
            would mean stacking two LSTMs together to form a `stacked LSTM`, 
            with the second LSTM taking in outputs of the first LSTM and 
            computing the final results. Default: 1 
        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`. 
            Default: ``True`` 
        batch_first: If ``True``, then the input and output tensors are provided 
            as (batch, seq, feature) 
        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each 
            LSTM layer except the last layer, with dropout probability equal to 
            :attr:`dropout`. Default: 0 
        bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False`` 
 
    Inputs: input, (h_0, c_0) 
        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features 
          of the input sequence. 
          The input can also be a packed variable length sequence. 
          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or 
          :func:`torch.nn.utils.rnn.pack_sequence` for details. 
        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor 
          containing the initial hidden state for each element in the batch. 
        - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor 
          containing the initial cell state for each element in the batch. 
 
          If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero. 
 
 
    Outputs: output, (h_n, c_n) 
        - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor 
          containing the output features `(h_t)` from the last layer of the LSTM, 
          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been 
          given as the input, the output will also be a packed sequence. 
 
          For the unpacked case, the directions can be separated 
          using ``output.view(seq_len, batch, num_directions, hidden_size)``, 
          with forward and backward being direction `0` and `1` respectively. 
          Similarly, the directions can be separated in the packed case. 
        - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor 
          containing the hidden state for `t = seq_len`. 
 
          Like *output*, the layers can be separated using 
          ``h_n.view(num_layers, num_directions, batch, hidden_size)`` and similarly for *c_n*. 
        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor 
          containing the cell state for `t = seq_len` 
 
    Attributes: 
        weight_ih_l[k] : the learnable input-hidden weights of the :math:`\text{k}^{th}` layer 
            `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)` 
        weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\text{k}^{th}` layer 
            `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)` 
        bias_ih_l[k] : the learnable input-hidden bias of the :math:`\text{k}^{th}` layer 
            `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)` 
        bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\text{k}^{th}` layer 
            `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)` 
 
    Examples:: 
 
        &gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2) 
        &gt;&gt;&gt; input = torch.randn(5, 3, 10) 
        &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) 
        &gt;&gt;&gt; c0 = torch.randn(2, 3, 20) 
        &gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0)) 
    &quot;&quot;&quot;</span><span class="s0"> 
 
    </span><span class="s1">def </span><span class="s0">__init__(self, *args, **kwargs): 
        super(LSTM, self).__init__(</span><span class="s3">'LSTM'</span><span class="s0">, *args, **kwargs) 
 
 
</span></pre>
</body>
</html>